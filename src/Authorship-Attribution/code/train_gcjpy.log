2023-05-04 21:29:14.456317: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-04 21:29:14.587400: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-04 21:29:15.135635: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-05-04 21:29:15.135691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-05-04 21:29:15.135697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
05/04/2023 21:29:16 - INFO - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
05/04/2023 21:29:18 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, attack_model_name='/home/liangshi/invisible_exp/CodeXGLUE/Authorship-Attribution/code/saved_models/gcjpy/checkpoint-attack-f1', block_size=510, cache_dir='', calc_asr=False, config_name='microsoft/codebert-base', device=device(type='cuda'), do_defense=False, do_eval=False, do_lower_case=False, do_test=False, do_train=True, epoch=20, eval_all_checkpoints=False, eval_batch_size=16, eval_data_file='../dataset/data_folder/useless_code/processed_clean_test/test.csv', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='microsoft/codebert-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, number_labels=66, output_dir='./saved_models/gcjpy', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, save_steps=50, save_total_limit=None, saved_model_name='token-defense', seed=123456, server_ip='', server_port='', start_epoch=0, start_step=0, test_data_file='../dataset/data_folder/useless_code_invisible_exp/processed_perturbated_training/train_label.csv', tokenizer_name='roberta-base', train_batch_size=16, train_data_file='../dataset/data_folder/useless_code/processed_perturbated_training/train.csv', warmup_steps=0, weight_decay=0.0)
05/04/2023 21:29:18 - INFO - __main__ -   Loading features from cached file ../dataset/data_folder/useless_code/processed_perturbated_training/cached_train
/home/liangshi/anaconda3/envs/pcb/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
05/04/2023 21:29:20 - INFO - __main__ -   ***** Running training *****
05/04/2023 21:29:20 - INFO - __main__ -     Num examples = 528
05/04/2023 21:29:20 - INFO - __main__ -     Num Epochs = 20
05/04/2023 21:29:20 - INFO - __main__ -     Instantaneous batch size per GPU = 16
05/04/2023 21:29:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16
05/04/2023 21:29:20 - INFO - __main__ -     Gradient Accumulation steps = 1
05/04/2023 21:29:20 - INFO - __main__ -     Total optimization steps = 660

 cached_features_file:  ../dataset/data_folder/useless_code/processed_perturbated_training/cached_train
  0%|          | 0/33 [00:00<?, ?it/s]epoch 0 loss 4.17867:   0%|          | 0/33 [00:01<?, ?it/s]epoch 0 loss 4.17867:   3%|▎         | 1/33 [00:01<00:39,  1.24s/it]epoch 0 loss 4.16722:   3%|▎         | 1/33 [00:01<00:39,  1.24s/it]epoch 0 loss 4.16722:   6%|▌         | 2/33 [00:01<00:28,  1.08it/s]epoch 0 loss 4.17474:   6%|▌         | 2/33 [00:02<00:28,  1.08it/s]epoch 0 loss 4.17474:   9%|▉         | 3/33 [00:02<00:24,  1.21it/s]